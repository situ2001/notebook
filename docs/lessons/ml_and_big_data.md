# 机器学习与大数据

这是一门选修课

## 模型选择

比如我们训练树叶识别的模型

过拟合：训练样本调为都有锯齿，把无锯齿的新样本扔进来，就会认为不是树叶（它误认为树叶是都有锯齿的）

欠拟合：训练样本调为绿色，扔新样本(树)，就把树当成树叶（认为绿色的都是）

---

模型选择的三个关键问题

1. 如何获得测试结果 -> 评估方法
2. 如果评估性能优劣 -> 性能度量
3. 如何判断实质差别 -> 比较检验

### 评估方法

Q: 如何获得数据集(dataset)

> 测试集可以由一些比较出名的benchmark里头翻出来找到，也可以是自己制作的数据。测试集应该要与训练集“互斥”，方法有
>
> 1. 留出法(hold-out)
> 2. 交叉验证法(cross validation)
> 3. 自助法(bootstrap)

- 留出法

就是在数据集中，留出了一小部分用于存放测试集($\approx1/5 - 1/3$)，要注意重复随机划分，要保持数据分布的一致性

- k-折交叉验证法

就是把数据集以一定比例分开为测试集与训练集，并且每一次训练和测试都交换一下其中一部分测试集与训练集的顺序。

``` shell
Data1-9 (Training) + Data10 (Testing) => result1
Data1-8 Data10 + Data9 => result2
...
... => result n
returning result = avg(result1, ..., result n)
```

- 自助法

这个数据的采样是可放回、可重复的采样。数据分布是有所改变的，训练集与原样本集同规模。

可以这么理解，你盲盒抽奖，抽到啥就放啥到新的数据集里，其中同一个物品，是有可能被多次抽到的。

包外估计(out-of-bag estimation)

$${\lim_{x\to \infty}}(1-m)^m=\frac{1}{e}\approx 0.368$$

- 调参
  
算法的参数：人工设定，也称之为超参数。模型的参数：一般是由学习设定。

算法模型的参数修改确定之后，要用训练集和验证集来重新训练模型。

涉及到：参数空间

### 性能度量

(performance measure)这是衡量模型泛化能力的评价标准，反映了任务需求

回归(regression)任务常用均方误差

$$E(f:D)=\frac{1}{m}\sum_{i=1}^{m}(f(x_{i})-y_{i}))^2$$

- 错误率与精度

$$E(f;D)=\frac{1}{m}\sum_{i=1}^{m}\prod (f(x_{i})\neq y_{i})$$

$$acc(f;D)=\frac{1}{m}\sum_{i=1}^{m}\prod (f(x_{i})\neq y_{i})$$

---

- 查准率与查全率

怎么理解?前面的字母是真是情况，后面的字母是预测的情况

||预测结果 正|反
|--|--|--
|真实情况 正|TP(真正例)|FN(假反例)
|反|FP|TN

查准就是用来判断，你预测是好的，但是实际上并不都是，这个反映的是模型预测的准确率。

查准率=$\frac{TP}{TP+FP}$

查全就是模型告诉我有这么多真正的好瓜，但是你只搞对了一部分瓜，（就是在实际真的情况中，你只找到了一部分真的，还有一部分是假的）。

查全率=$\frac{TP}{TP+FN}$

---

- PR图与BEP

这图就是一个二维坐标系，一个轴为查准率，一个为查全率，对一个学习器的预测结果进行多次测量之后得出的曲线。其中BEP就是平衡点，基于的是`查准率=查全率`的一条曲线

- F1
  
平时我们用的比BEP更多的就是F1度量了，有一个公式是这样的，它可以表示对查全率与查准率的不同偏好

$$F_{\beta}=\frac{(1+\beta ^2) *P*R}{(\beta ^2 *P)+R}$$

其中$\beta >1$，查全率有更大影响，反之

- 宏微指标

宏(macro-)与微(micro-)，应该是指的宏观微观吧，一个是求全部的，一个是对一个一个地求

- ROC 与 AUC

AUC: Area Under the ROC(Receiver Operating Characteristic) Curve

- 非均等代价

犯下不同的错误往往会造成不同的损失。此时就需要考虑非均等代价了(unequal cost)，与此还有代价敏感(cost-sensitive)公式

TODO

### 比较检验

机器学习 => 概率近似准确

统计假设检验为学习器性能比较提供了重要依据

FriedMan检验图

---

### 误差

在此之前先放一个表格

|sign|meaning
|:--:|--
|$x$|测试样本
|$D$|数据集
|$y_{D}$|x在数据集中的标记
|$y$|x的真实标记
|$f$|用训练集$D$训练得到的模型
|$f(x;D)$|由$D$训练得到的模型$f$对$x$的输出
|$\bar{f}(x)$|由模型$f$对$x$的期望预测输出

偏差-方差分解(bias-variance decomposition)

$$E(f:D)={bias}^{2}(x)+var(x)+\epsilon ^2$$

其中

${bias}^{2}(x)$ - 期望输出与真实输出的差别(算法本身的拟合能力)

$${bias}^{2}=(\bar{f}(x)-y)^2$$

$var(x)$ - 同样大小的训练集的变动所导致的性能变化(数据的扰动)

$$var(x)=\mathbb{E}_{D}[(f(x;D)-\bar{f}(x))^{2}]$$

$\epsilon^2$ - (噪声)当前任务上任何学习算法所能达到的期望泛化误差的下界(学习问题本身的难度)

$$\epsilon^{2}=\mathbb{E}_{D}[(y_{D}-y)^2]$$

这就是泛化误差了，泛化性能是由学习算法的能力、数据的充分性以及学习任务本身的难度共同决定

同时也提到了: 偏差-方差窘境(bias-variance dilemma)，即训练的强度不同，泛化误差就会出现不同参数的主导阶段，就是偏差主导和方差主导。

---
